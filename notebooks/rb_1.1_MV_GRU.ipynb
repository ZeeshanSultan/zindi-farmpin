{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate GRU\n",
    "\n",
    "**Changes**:\n",
    "\n",
    "* More layers\n",
    "* More units\n",
    "* More features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from config import interim_data_dir, raw_data_dir\n",
    "\n",
    "from src.utils import read_shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 13:27:35.209162 140655377872256 deprecation_wrapper.py:119] From /home/jupyter/miniconda3/envs/zindi_farmpin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0724 13:27:35.210292 140655377872256 deprecation_wrapper.py:119] From /home/jupyter/miniconda3/envs/zindi_farmpin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0724 13:27:35.211174 140655377872256 deprecation_wrapper.py:119] From /home/jupyter/miniconda3/envs/zindi_farmpin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0724 13:27:35.242944 140655377872256 deprecation_wrapper.py:119] From /home/jupyter/miniconda3/envs/zindi_farmpin/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from config import interim_data_dir, raw_data_dir\n",
    "\n",
    "from src.utils import read_shapefile\n",
    "\n",
    "from config import processed_data_dir\n",
    "\n",
    "dataset_version = 'v4'\n",
    "\n",
    "train_features_df = pd.read_csv(processed_data_dir/'VI_datasets'/dataset_version/'train.csv', index_col=0)\n",
    "test_features_df = pd.read_csv(processed_data_dir/'VI_datasets'/dataset_version/'test.csv', index_col=0)\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imput missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_nan_inf(df):\n",
    "    \"\"\"\n",
    "    Impute bad values (nan and inf) using\n",
    "    the median of that feature on that time stamp\n",
    "    for all farms\n",
    "    \n",
    "    TODO: This can be improved\n",
    "    \"\"\"\n",
    "    \n",
    "    # replace all nans variants with np.nan\n",
    "    df = df.replace([np.nan, None, np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # replace all nans with the median value of all farms on that timestamp\n",
    "    for ts, ts_df in df.groupby('time'):\n",
    "        df.loc[ts_df.index] = ts_df.fillna(ts_df.median())\n",
    "        \n",
    "    return df\n",
    "        \n",
    "train_features_df = impute_nan_inf(train_features_df)\n",
    "test_features_df = impute_nan_inf(test_features_df)\n",
    "\n",
    "train_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Features Dataset\n",
    "\n",
    "Neural Nets are sensitive to scale, but we want to keep the variance in the features. So we will MinMax scale each feature on a specific time step over all farms. \n",
    "\n",
    "I.e - the network will see any number of variables at a time, but all of them will be on the same scale (0 - 1) and the variance between farms will be kept the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies to compare before / after normalization\n",
    "train_copy = train_features_df.copy()\n",
    "test_copy = test_features_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "times = train_features_df['time'].unique()\n",
    "feature_cols = [col for col in test_features_df.columns if col not in ['farm_id','time']]\n",
    "\n",
    "for ts in times:\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    train_ts_df = train_copy[train_copy.time == ts]\n",
    "    test_ts_df = test_copy[test_copy.time == ts]\n",
    "    \n",
    "    train_features_df.loc[train_ts_df.index, feature_cols] = scaler.fit_transform(train_ts_df[feature_cols])\n",
    "    test_features_df.loc[test_ts_df.index, feature_cols] = scaler.transform(test_ts_df[feature_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dataset for MVTS Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(features_df):\n",
    "    \"\"\"\n",
    "    Create a data array in the shape (n_ids, n_time, n_features)\n",
    "    \n",
    "    If it is the training set, also return dictionary mapping index\n",
    "    in dataset to correct label.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_ids = features_df.farm_id.nunique()\n",
    "\n",
    "    feature_cols = [col for col in features_df.columns if col not in ['farm_id','y','time']]\n",
    "    \n",
    "    n_features = len(feature_cols)\n",
    "\n",
    "    data = np.ndarray((n_ids, 11, n_features))\n",
    "    \n",
    "    classes = {}\n",
    "    for i, (farm_id, farm_df) in enumerate(features_df.groupby(['farm_id'])):\n",
    "\n",
    "        data[i,:,:] = farm_df[feature_cols].values\n",
    "        \n",
    "        if 'y' in features_df.columns:\n",
    "            c = farm_df['y'].unique()\n",
    "            classes[i] = c[0]\n",
    "            \n",
    "    return data, classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels_dict = transform_dataset(train_features_df)\n",
    "test_data, _ = transform_dataset(test_features_df)\n",
    "\n",
    "print('Train data:\\t\\t',train_data.shape)\n",
    "print('Test data:\\t\\t',test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_df.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Modelling Process\n",
    "\n",
    "### Create validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Copy for later\n",
    "data = train_data.copy()\n",
    "\n",
    "train_labels = list(train_labels_dict.values())\n",
    "train_indexes = list(train_labels_dict.keys())\n",
    "train_idx, val_idx, train_labels, val_labels = train_test_split(train_indexes, train_labels, stratify=train_labels, test_size=0.2)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labels_enc = le.fit_transform(np.array(train_labels).reshape(-1,1))\n",
    "val_labels_enc = le.transform(np.array(val_labels).reshape(-1,1))\n",
    "\n",
    "train_data = data[train_idx,:, :]\n",
    "val_data = data[val_idx,:, :]\n",
    "\n",
    "print('train data : ',train_data.shape)\n",
    "print('val data : ',val_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from src.visualization.tf_viz import PlotLosses\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', verbose=1, patience=100)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=50, verbose=1)\n",
    "\n",
    "class_weight = 1-train_features_df.y.value_counts(normalize=True)\n",
    "\n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "def get_model(n_features):\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.GRU(8, input_shape=(11, n_features), dropout=0.2, kernel_regularizer=keras.regularizers.l2(0.02)))\n",
    "    model.add(layers.Flatten())\n",
    "#     model.add(layers.Dense(18 ,activation='relu', kernel_regularizer=keras.regularizers.l2(0.02)))\n",
    "    model.add(layers.Dense(9, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = get_model(train_data.shape[-1])\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=0.003)\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "# optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=keras.metrics.sparse_categorical_crossentropy,\n",
    "              metrics=[keras.metrics.sparse_categorical_crossentropy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, \n",
    "          train_labels_enc,\n",
    "          batch_size=64,\n",
    "          validation_data=(val_data, val_labels_enc), \n",
    "          class_weight=class_weight,\n",
    "          verbose=2,\n",
    "          epochs=2000, \n",
    "          workers=4, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[plot_losses, es, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "val_preds = model.predict(val_data)\n",
    "train_preds = model.predict(train_data)\n",
    "print('val loss: ',log_loss(val_labels_enc, val_preds))\n",
    "print('train loss: ',log_loss(train_labels_enc, train_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from src.visualization.visualize import print_confusion_matrix\n",
    "\n",
    "class_labels = pd.read_csv(raw_data_dir / 'crop_id_list.csv', index_col=0).sort_index().crop\n",
    "\n",
    "val_preds = model.predict_classes(val_data)\n",
    "\n",
    "cm = confusion_matrix(val_labels_enc, val_preds)\n",
    "print_confusion_matrix(cm, class_names=class_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First retrain on all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.vstack([train_data, val_data])\n",
    "all_labels = np.array(list(train_labels_enc) + list(val_labels_enc))\n",
    "\n",
    "class_weight = (1 - pd.Series(all_labels).value_counts(normalize=True)).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(all_data.shape[-1])\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=keras.metrics.sparse_categorical_crossentropy,\n",
    "              metrics=[keras.metrics.sparse_categorical_crossentropy])\n",
    "\n",
    "model.fit(all_data, \n",
    "          all_labels,\n",
    "          batch_size=64, \n",
    "          validation_split=0.1,\n",
    "          class_weight=class_weight,\n",
    "          verbose=2,\n",
    "          epochs=350, \n",
    "          workers=4, \n",
    "          use_multiprocessing=True,\n",
    "          callbacks=[plot_losses, es, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub(predictions, ids_list):\n",
    "    \n",
    "    sub_df = pd.read_csv(raw_data_dir / 'sample_submission_fixed.csv')\n",
    "\n",
    "    new_sub = sub_df.copy().set_index('field_id')\n",
    "\n",
    "    new_sub.loc[ids_list] = predictions\n",
    "    \n",
    "    return new_sub\n",
    "\n",
    "predictions = model.predict(test_data)\n",
    "test_ids = sorted(list(test_features_df.farm_id.unique()))\n",
    "sub_df = make_sub(predictions, test_ids)\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import subs_dir\n",
    "\n",
    "sub_df.to_csv(subs_dir / 'gru_vi_v3_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
